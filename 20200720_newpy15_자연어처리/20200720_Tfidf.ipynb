{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595209713307",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\n"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() # 텐서플로우 버전 1로 낮추기 (버전 2 기능 사용불가하게 만들기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "자연어 처리를 위한 라이브러리 설치\n",
    "1) 텐서플로우\n",
    "2) 케라스\n",
    "3) 젠심\n",
    "4) 사이킷런\n",
    "5) nltk (영문자연어처리) -> nltk.download\n",
    "6) konlpy\n",
    "7) jpype\n",
    "* 설치된 라이브러리 확인 : pip list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastery', 'shop', '.']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastery shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastery', 'shop', '.']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastery shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastery', 'shop']\n"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastery shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화\n",
    "text1 = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "text2 = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['His barber kept his word.',\n 'But keeping such a huge secret to himself was driving him crazy.',\n 'Finally, the barber went up a mountain and almost to the edge of a cliff.',\n 'He dug a hole in the midst of some reeds.',\n 'He looked about, to mae sure no one was near.']"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text1)\n",
    "sent_tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('I', 'PRP'),\n ('am', 'VBP'),\n ('actively', 'RB'),\n ('looking', 'VBG'),\n ('for', 'IN'),\n ('Ph.D.', 'NNP'),\n ('students', 'NNS'),\n ('.', '.'),\n ('and', 'CC'),\n ('you', 'PRP'),\n ('are', 'VBP'),\n ('a', 'DT'),\n ('Ph.D', 'NNP'),\n ('student', 'NN'),\n ('.', '.')]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag #품사확인\n",
    "\n",
    "imsi = word_tokenize(text1)\n",
    "pos_tag(imsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['공부', '우리', '취업']"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "from konlpy.tag import Okt # 옛날 Twitter 클래스\n",
    "\n",
    "okt = Okt()\n",
    "okt.morphs(\"열심히 공부한 우리, 취업에 성공합시다\") #형태소 추출\n",
    "okt.pos(\"열심히 공부한 우리, 취업에 성공합시다\") #품사\n",
    "okt.nouns(\"열심히 공부한 우리, 취업에 성공합시다\") #명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "complete\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'complet'"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# 어간추출(stem, 의미), 접사(부가적인 의미)\n",
    "# dogs dog(어간) -s(접사)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wn = WordNetLemmatizer() #표제어 추출 (품사정보유지)\n",
    "words = ['loves','dies','complete']\n",
    "print(wn.lemmatize(words[2]))\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer() #어간추출 (품사정보상실)\n",
    "ps.stem(words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "ex = \"Family is not an important thing. It's everything.\"\n",
    "imsi = word_tokenize(ex)\n",
    "\n",
    "result = []\n",
    "for i in imsi:\n",
    "    if i not in stopwords.words('english'):\n",
    "        result.append(i)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ranks.nl/stopwords/korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Family', 'is', 'not', 'an', 'important', 'thing', 'It', 's', 'everything']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer #정규표현식 토크나이저\n",
    "tok = RegexpTokenizer('[\\w]+') #\\w = [a-zA-Z0-9]\n",
    "print(tok.tokenize(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
    }
   ],
   "source": [
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "len('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(text) # 11개의 문장 각각에 대해 단어토큰화 수행\n",
    "# 1) 불용어 제거\n",
    "# 2) 단어길이가 2 이하면 제거\n",
    "# 3) 모든 단어는 소문자화\n",
    "\n",
    "vocab = {}\n",
    "mylist = []\n",
    "\n",
    "for i in text:\n",
    "    j = text_to_word_sequence(i)\n",
    "    sent = []\n",
    "    for k in j:\n",
    "        if k not in stopwords.words('english'):\n",
    "            if len(k)>2:\n",
    "                sent.append(k)\n",
    "                if k not in vocab:\n",
    "                    vocab[k] = 0\n",
    "                vocab[k] += 1\n",
    "    mylist.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
    }
   ],
   "source": [
    "print(mylist)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = sorted(vocab.items(), key=lambda x:x[1] , reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('barber', 8),\n ('secret', 6),\n ('huge', 5),\n ('kept', 4),\n ('person', 3),\n ('word', 2),\n ('keeping', 2),\n ('good', 1),\n ('knew', 1),\n ('driving', 1),\n ('crazy', 1),\n ('went', 1),\n ('mountain', 1)]"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
    }
   ],
   "source": [
    "wti = {}\n",
    "i = 0\n",
    "\n",
    "for word,f in vs:\n",
    "    if f > 1 : # 단어빈도가 2 이상인 경우\n",
    "        i += 1\n",
    "        wti[word] = i\n",
    "\n",
    "print(wti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = okt.morphs(\"오전에는 졸았지만, 오후에는 열심히 하겠다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'오전': 0, '에는': 1, '졸았지만': 2, ',': 3, '오후': 4, '열심히': 5, '하겠다': 6}\n"
    }
   ],
   "source": [
    "# 토큰 단위로 딕셔너리에 저장\n",
    "word2index = {}\n",
    "for v in token:\n",
    "    # v 토큰이 word2index에 없다면\n",
    "    if v not in word2index.keys():\n",
    "        word2index[v] = len(word2index)\n",
    "\n",
    "print(word2index) # 각 형태소에 인덱스 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 0, 0, 0, 1, 0, 0, 0]\n"
    }
   ],
   "source": [
    "def ohe1(w,word2index):\n",
    "    arr = []\n",
    "    for i in word2index.keys():\n",
    "        if w==i:\n",
    "            arr.append(1)\n",
    "        arr.append(0)\n",
    "    return arr\n",
    "\n",
    "print(ohe1('오후', word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 0, 0, 0, 1, 0, 0]\n"
    }
   ],
   "source": [
    "def ohe(w,word2index):\n",
    "    ohv = [0]*len(word2index)\n",
    "    idx = word2index[w]\n",
    "    ohv[idx] = 1\n",
    "    return ohv\n",
    "print(ohe('오후', word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<keras_preprocessing.text.Tokenizer object at 0x00000277F1A29648>\n"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "text = \"\"\"나랑 막걸리 마시러 갈래 안주는 파전 갈래 갈래 파전 최고야\"\"\"\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: '갈래', 2: '파전', 3: '나랑', 4: '막걸리', 5: '마시러', 6: '안주는', 7: '최고야'}"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "t.word_index # 가장 많이 등장한 단어부터 역순으로 출력\n",
    "t.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "enc = t.texts_to_sequences(['술 마시러 갈래 갈꺼야 말꺼야 갈래'])[0] # 코퍼스에 존재하는 단어의 인덱스 출력\n",
    "to_categorical(enc) # 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다'"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "# bag of words : 각 단어에 인덱스 부여 -> 등장횟수 저장\n",
    "import re\n",
    "\n",
    "# 정규표현식과 sub로 마침표 없애기\n",
    "token = re.sub(\"[.]\",\"\", \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "# token = re.sub(\"(\\.)\",\"\", \"정부가 발표하는 물가 상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "token\n",
    "# re.sub(패턴,교체문자,데이터)\n",
    "# re.sub(\"대한민국\",\"한국\",\"대한민국 우리나라\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'정부가 발표하는 물가 상승률과 소비자가 느끼는 물가상승률은 다르다'"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['정부', '가', '발표', '하는', '물가상승률', '과', '소비자', '가', '느끼는', '물가상승률', '은', '다르다']"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "token = okt.morphs(token)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
    }
   ],
   "source": [
    "bow = []\n",
    "word2index = {}\n",
    "for v in token:\n",
    "    if v not in word2index.keys():\n",
    "        word2index[v] = len(word2index)\n",
    "        bow.insert(len(word2index)-1,1) #0으로 각 자리를 초기화\n",
    "    else:\n",
    "        idx = word2index.get(v)\n",
    "        bow[idx] += 1\n",
    "        # print(word2index)\n",
    "        # print(\"=\"*50)\n",
    "print(word2index)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'you': 3, 'know': 1, 'want': 2, 'your': 4, 'because': 0}\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "#vec = CountVectorizer()\n",
    "vec = CountVectorizer(stop_words=['love'])\n",
    "# 각 단어의 빈도수\n",
    "vec.fit_transform(corpus).toarray()\n",
    "# 각 단어의 인덱스\n",
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['바나나', '노란', '좋아요', '사과', '싶은', '과일이', '길고', '먹고', '저는']"
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "docs = [\n",
    "    '먹고 싶은 사과',\n",
    "    '먹고 싶은 바나나',\n",
    "    '길고 노란 바나나 바나나',\n",
    "    '저는 과일이 좋아요'\n",
    "]\n",
    "\n",
    "voc = list(set(w for doc in docs for w in doc.split()))\n",
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['과일이', '길고', '노란', '먹고', '바나나', '사과', '싶은', '저는', '좋아요']"
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "voc.sort()\n",
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "\n",
    "n = len(docs)\n",
    "def tfv(t,d):\n",
    "    return d.count(t) # d문서 내에서 t단어의 등장 횟수\n",
    "def idf(t):\n",
    "    df=0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(n/(df+1))\n",
    "def tfidf(t,d):\n",
    "    return tfv(t,d)*idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DTM 구현\"\"\"\n",
    "#   과일이...\n",
    "#1      0\n",
    "#2      0\n",
    "#3      1\n",
    "#4      0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n0    0   0   0   1    0   1   1   0    0\n1    0   0   0   1    1   0   1   0    0\n2    0   1   1   0    2   0   0   0    0\n3    1   0   0   0    0   0   0   1    1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>과일이</th>\n      <th>길고</th>\n      <th>노란</th>\n      <th>먹고</th>\n      <th>바나나</th>\n      <th>사과</th>\n      <th>싶은</th>\n      <th>저는</th>\n      <th>좋아요</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "res = []\n",
    "for i in range(n):\n",
    "    res.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(voc)):\n",
    "        t = voc[j]\n",
    "        res[-1].append(tfv(t,d))\n",
    "mytf = pd.DataFrame(res,columns=voc)\n",
    "mytf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          IDF\n과일이  0.693147\n길고   0.693147\n노란   0.693147\n먹고   0.287682\n바나나  0.287682\n사과   0.693147\n싶은   0.287682\n저는   0.693147\n좋아요  0.693147",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>과일이</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>길고</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>노란</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>먹고</th>\n      <td>0.287682</td>\n    </tr>\n    <tr>\n      <th>바나나</th>\n      <td>0.287682</td>\n    </tr>\n    <tr>\n      <th>사과</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>싶은</th>\n      <td>0.287682</td>\n    </tr>\n    <tr>\n      <th>저는</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>좋아요</th>\n      <td>0.693147</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "# 각 단어에 대한 idf를 출력\n",
    "res = []\n",
    "for j in range(len(voc)):\n",
    "    t = voc[j]\n",
    "    res.append(idf(t))\n",
    "idf_ = pd.DataFrame(res,index=voc, columns = ['IDF'])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n\n         저는       좋아요  \n0  0.000000  0.000000  \n1  0.000000  0.000000  \n2  0.000000  0.000000  \n3  0.693147  0.693147  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>과일이</th>\n      <th>길고</th>\n      <th>노란</th>\n      <th>먹고</th>\n      <th>바나나</th>\n      <th>사과</th>\n      <th>싶은</th>\n      <th>저는</th>\n      <th>좋아요</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.287682</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>0.693147</td>\n      <td>0.000000</td>\n      <td>0.575364</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.693147</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>0.693147</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(n):\n",
    "    res.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(voc)):\n",
    "        t = voc[j]\n",
    "        res[-1].append(tfidf(t,d))\n",
    "\n",
    "mytfidf = pd.DataFrame(res, columns = voc)\n",
    "mytfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0 1 0 1 0 1 0 2]\n [0 0 1 0 0 0 0 1]\n [1 0 0 0 1 0 1 0]]\n{'you': 7, 'know': 1, 'want': 5, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
    }
   ],
   "source": [
    "############### 여기까진 지우개로 지우세요 #################\n",
    "\n",
    "corpus = [\n",
    "    \"you know i want you love\",\n",
    "    \"I like you\",\n",
    "    \"what should I do\"\n",
    "]\n",
    "\n",
    "# DTM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) #tf\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        do      know      like      love   should      want     what       you\n0  0.00000  0.433816  0.000000  0.433816  0.00000  0.433816  0.00000  0.659857\n1  0.00000  0.000000  0.795961  0.000000  0.00000  0.000000  0.00000  0.605349\n2  0.57735  0.000000  0.000000  0.000000  0.57735  0.000000  0.57735  0.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>do</th>\n      <th>know</th>\n      <th>like</th>\n      <th>love</th>\n      <th>should</th>\n      <th>want</th>\n      <th>what</th>\n      <th>you</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00000</td>\n      <td>0.433816</td>\n      <td>0.000000</td>\n      <td>0.433816</td>\n      <td>0.00000</td>\n      <td>0.433816</td>\n      <td>0.00000</td>\n      <td>0.659857</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.795961</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.605349</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "a = tfidfv.transform(corpus).toarray()\n",
    "pd.DataFrame(a, columns = sorted(tfidfv.vocabulary_.keys()))"
   ]
  }
 ]
}