{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595981905167",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '해보지 않으면 해낼 수 없다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "원문 :  해보지 않으면 해낼 수 없다\n토큰화 :  ['해보지', '않으면', '해낼', '수', '없다']\n"
    }
   ],
   "source": [
    "# 텍스트 -> 토큰화\n",
    "\n",
    "result = text_to_word_sequence(text)\n",
    "print(\"원문 : \",text)\n",
    "print(\"토큰화 : \",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n",
    "        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n",
    "        '토큰화 한 결과는 딥러닝에서 사용할 수 있습니다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화', 1), ('한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n"
    }
   ],
   "source": [
    "print(token.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3\ndefaultdict(<class 'int'>, {'텍스트의': 2, '나누어': 1, '각': 1, '단어를': 1, '먼저': 1, '토큰화합니다': 1, '딥러닝에서': 2, '인식됩니다': 1, '토큰화해야': 1, '단어로': 1, '결과는': 1, '수': 1, '한': 1, '사용할': 1, '있습니다': 1, '토큰화': 1})\n{'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화': 11, '한': 12, '결과는': 13, '사용할': 14, '수': 15, '있습니다': 16}\n"
    }
   ],
   "source": [
    "print(token.document_count) # 문장개수\n",
    "print(token.word_docs) # 각 단어가 포함된 문장의 갯수\n",
    "print(token.word_index) # 각 단어가 포함된 문장의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['너무 재밌네요', \"최고에요\", '참 잘 만든 영화에요',\n",
    "        '추천하고 싶은 영화입니다','한번 더 보고싶네요','글쎄요',\n",
    "        '별로에요','생각보다 지루하네요','연기가 어색해요','재미없어요']\n",
    "classes = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'너무': 1, '재밌네요': 2, '최고에요': 3, '참': 4, '잘': 5, '만든': 6, '영화에요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로에요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n패딩결과 :  [[ 0  0  1  2]\n [ 0  0  0  3]\n [ 4  5  6  7]\n [ 0  8  9 10]\n [ 0 11 12 13]\n [ 0  0  0 14]\n [ 0  0  0 15]\n [ 0  0 16 17]\n [ 0  0 18 19]\n [ 0  0  0 20]]\n"
    }
   ],
   "source": [
    "print(token.word_index)\n",
    "x = token.texts_to_sequences(docs)\n",
    "# x에 저장된 서로 다른 단어의개수(배열길이)를 4로 통일(패딩)\n",
    "paddedX = pad_sequences(x,4)\n",
    "print(\"패딩결과 : \", paddedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  딥러닝모델 작성\n",
    "wordSize = len(token.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding(wordSize,8,input_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 감성분석 (긍정/부정)\n",
    "model = Sequential()\n",
    "model.add(Embedding(wordSize,8,input_length=4))\n",
    "# Embedding(단어개수(voc),임베딩벡터크기(8차원),입력시퀀스길이))\n",
    "# 21차원 -> 임베딩(단어>밀집벡터) -> 8차원\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.fit(paddedX,classes,epochs=20)\n",
    "print(\"정확도 : %.4f\" % (model.evaluate(paddedX, classes)[1]))\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "단어 종류 : 21가지-> 벡터 공간 -> 21차원 -> 임베딩 => 2차원\n",
    "100000...0000 => 1.5, 3.7\n",
    "100000...0001 => 1.0, 2.7\n",
    "임베딩 : 단어 -> 밀집 벡터 (Dense vector ))\n",
    "신경말\n",
    "ex) 원핫인코딩 단어 벡터의 차원이 50000 차원 => 임베딩 => 2차원\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM을 이용한 뉴스분류기\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(xTrain, yTrain), (xTest, yTest) = reuters.load_data(num_words=1000, test_split= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "카테고리: 46\n학습데이터: 8982\n테스트데이터: 2246\n[1, 2, 699, 2, 2, 56, 2, 2, 9, 56, 2, 2, 81, 5, 2, 57, 366, 737, 132, 20, 2, 7, 2, 49, 2, 2, 2, 2, 699, 2, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2, 2, 2, 775, 7, 48, 34, 191, 44, 35, 2, 505, 17, 12]\n3\n"
    }
   ],
   "source": [
    "category=np.max(yTrain)+1 #뉴스 카테고리(46가지), 0~45 => 1~46\n",
    "print(\"카테고리:\", category)\n",
    "print(\"학습데이터:\", len(xTrain)) #8982 건\n",
    "print(\"테스트데이터:\", len(xTest)) #2246 건\n",
    "print(xTrain[1]) #길이 87인 뉴스 기사\n",
    "print(yTrain[0]) #3 카테고리 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=pad_sequences(xTrain,maxlen=100)\n",
    "xTest=pad_sequences(xTest,maxlen=100)\n",
    "yTrain=np_utils.to_categorical(yTrain)\n",
    "yTest=np_utils.to_categorical(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 생성\n",
    "model=Sequential()\n",
    "model.add(Embedding(1000, 100))\n",
    "model.add(LSTM(100, activation='tanh'))\n",
    "model.add(Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/20\n90/90 [==============================] - 8s 91ms/step - loss: 2.5978 - accuracy: 0.3477 - val_loss: 2.4011 - val_accuracy: 0.3620\nEpoch 2/20\n90/90 [==============================] - 8s 90ms/step - loss: 2.1173 - accuracy: 0.4637 - val_loss: 2.0200 - val_accuracy: 0.4982\nEpoch 3/20\n90/90 [==============================] - 8s 91ms/step - loss: 1.9110 - accuracy: 0.5184 - val_loss: 1.8319 - val_accuracy: 0.5490\nEpoch 4/20\n90/90 [==============================] - 8s 85ms/step - loss: 1.7411 - accuracy: 0.5561 - val_loss: 1.7201 - val_accuracy: 0.5637\nEpoch 5/20\n90/90 [==============================] - 8s 85ms/step - loss: 1.6549 - accuracy: 0.5856 - val_loss: 1.6539 - val_accuracy: 0.5819\nEpoch 6/20\n90/90 [==============================] - 8s 84ms/step - loss: 1.5927 - accuracy: 0.5973 - val_loss: 1.6180 - val_accuracy: 0.5841\nEpoch 7/20\n90/90 [==============================] - 7s 81ms/step - loss: 1.4751 - accuracy: 0.6304 - val_loss: 1.5749 - val_accuracy: 0.6153\nEpoch 8/20\n90/90 [==============================] - 7s 80ms/step - loss: 1.3695 - accuracy: 0.6562 - val_loss: 1.4376 - val_accuracy: 0.6407\nEpoch 9/20\n90/90 [==============================] - 7s 80ms/step - loss: 1.3014 - accuracy: 0.6687 - val_loss: 1.4019 - val_accuracy: 0.6500\nEpoch 10/20\n90/90 [==============================] - 8s 84ms/step - loss: 1.2105 - accuracy: 0.6915 - val_loss: 1.4017 - val_accuracy: 0.6505\nEpoch 11/20\n90/90 [==============================] - 8s 91ms/step - loss: 1.1445 - accuracy: 0.7067 - val_loss: 1.3239 - val_accuracy: 0.6594\nEpoch 12/20\n90/90 [==============================] - 8s 91ms/step - loss: 1.0808 - accuracy: 0.7222 - val_loss: 1.2635 - val_accuracy: 0.6763\nEpoch 13/20\n90/90 [==============================] - 8s 87ms/step - loss: 1.0136 - accuracy: 0.7376 - val_loss: 1.2981 - val_accuracy: 0.6763\nEpoch 14/20\n90/90 [==============================] - 8s 87ms/step - loss: 0.9803 - accuracy: 0.7516 - val_loss: 1.2364 - val_accuracy: 0.6897\nEpoch 15/20\n90/90 [==============================] - 8s 85ms/step - loss: 0.9289 - accuracy: 0.7660 - val_loss: 1.1873 - val_accuracy: 0.7057\nEpoch 16/20\n90/90 [==============================] - 8s 90ms/step - loss: 0.8659 - accuracy: 0.7817 - val_loss: 1.1921 - val_accuracy: 0.7093\nEpoch 17/20\n90/90 [==============================] - 8s 92ms/step - loss: 0.8234 - accuracy: 0.7960 - val_loss: 1.1867 - val_accuracy: 0.7097\nEpoch 18/20\n90/90 [==============================] - 7s 83ms/step - loss: 0.7854 - accuracy: 0.8033 - val_loss: 1.2078 - val_accuracy: 0.7106\nEpoch 19/20\n90/90 [==============================] - 8s 84ms/step - loss: 0.7527 - accuracy: 0.8114 - val_loss: 1.1995 - val_accuracy: 0.7124\nEpoch 20/20\n90/90 [==============================] - 7s 83ms/step - loss: 0.7210 - accuracy: 0.8200 - val_loss: 1.2112 - val_accuracy: 0.7142\n"
    }
   ],
   "source": [
    "history=model.fit(xTrain, yTrain, batch_size=100, epochs=20,\n",
    "         validation_data=(xTest, yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LSTM : 긴 시퀀스 기억 계층\n",
    "ex) LSTM(3, input_dim=1)\n",
    "        출력    입력속성수\n",
    "    LSTM(3, input_dim=1, input_length=4)\n",
    "input_length : 시퀀스데이터의 길이\n",
    "시퀀스 출력여부(True:매 입력출력, False:맨마지막에만 출력))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hihell 까지 넣으면 hihello 출력하는 모델 만들기\n",
    "\n",
    "sample = 'hihello'\n",
    "charSet = list(set(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'e': 0, 'o': 1, 'h': 2, 'i': 3, 'l': 4}"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "charDic = {w:i for i,w in enumerate(charSet)}\n",
    "charDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xStr = sample[:-1]\n",
    "yStr = sample[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadim = len(charSet) #VOC 출력결과로 나와야 하는 문자의 종류의 수\n",
    "numClasses = len(charSet) #5가지 문자 종류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[3, 2, 0, 4, 4, 1]"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "x = [charDic[c] for c in xStr]\n",
    "x # hihell의 인덱스 [2, 3, 2, 0, 4, 4]\n",
    "y = [charDic[c] for c in yStr]\n",
    "y # ihello의 인덱스 [3, 2, 0, 4, 4, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1.]]\n(6, 5)\n[[0. 0. 0. 1. 0.]\n [0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1.]\n [0. 1. 0. 0. 0.]]\n(6, 5)\n"
    }
   ],
   "source": [
    "# 원핫인코딩\n",
    "x = np_utils.to_categorical(x, num_classes=numClasses)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "y = np_utils.to_categorical(y, num_classes=numClasses)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 6, 5)"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "# x.shape # (6,5) 6:입력길이, 5:차원\n",
    "x = np.reshape(x,(-1, len(x), datadim))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 6, 5)"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "y = np.reshape(y,(-1, len(y), datadim))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_3 (LSTM)                (None, 6, 640)            1653760   \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 6, 320)            1230080   \n_________________________________________________________________\nlstm_5 (LSTM)                (None, 6, 80)             128320    \n_________________________________________________________________\nlstm_6 (LSTM)                (None, 6, 20)             8080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 6, 5)              105       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 6, 5)              0         \n=================================================================\nTotal params: 3,020,345\nTrainable params: 3,020,345\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = Sequential()\n",
    "\n",
    "# 좋지 않은 모델 구조\n",
    "# model.add(LSTM( 5, # 출력되어야할 문자의 종류\n",
    "#                 input_shape=(6,5), # timesteps(6글자입력) / dataDim(입력문자차원(종류))\n",
    "#                 return_sequences=True))\n",
    "\n",
    "# 계층을 딥하고 와이드하게 설계\n",
    "model.add(LSTM( 5*128, input_shape=(6,5), return_sequences=True ))\n",
    "model.add(LSTM( 5*64, return_sequences=True ))\n",
    "model.add(LSTM( 5*16, return_sequences=True ))\n",
    "model.add(LSTM( 5*4, return_sequences=True ))\n",
    "model.add(Dense( 5 ))\n",
    "model.add(Activation('softmax')) #여러개의 문자중에 하나 출력 : softmax\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.6103 - accuracy: 0.1667\nEpoch 2/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.5992 - accuracy: 0.5000\nEpoch 3/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.5862 - accuracy: 0.5000\nEpoch 4/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.5678 - accuracy: 0.5000\nEpoch 5/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.5408 - accuracy: 0.3333\nEpoch 6/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.5032 - accuracy: 0.1667\nEpoch 7/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.4585 - accuracy: 0.1667\nEpoch 8/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.4174 - accuracy: 0.1667\nEpoch 9/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.3875 - accuracy: 0.1667\nEpoch 10/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.3675 - accuracy: 0.1667\nEpoch 11/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.3488 - accuracy: 0.1667\nEpoch 12/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.3232 - accuracy: 0.1667\nEpoch 13/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.2926 - accuracy: 0.3333\nEpoch 14/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.2671 - accuracy: 0.5000\nEpoch 15/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.2429 - accuracy: 0.3333\nEpoch 16/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.2144 - accuracy: 0.5000\nEpoch 17/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.1874 - accuracy: 0.5000\nEpoch 18/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.1629 - accuracy: 0.5000\nEpoch 19/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.1373 - accuracy: 0.5000\nEpoch 20/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.1121 - accuracy: 0.5000\nEpoch 21/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.0913 - accuracy: 0.5000\nEpoch 22/200\n1/1 [==============================] - 0s 2ms/step - loss: 1.0666 - accuracy: 0.5000\nEpoch 23/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.0462 - accuracy: 0.5000\nEpoch 24/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.0240 - accuracy: 0.5000\nEpoch 25/200\n1/1 [==============================] - 0s 1ms/step - loss: 1.0043 - accuracy: 0.5000\nEpoch 26/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.9836 - accuracy: 0.5000\nEpoch 27/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.9623 - accuracy: 0.5000\nEpoch 28/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.9452 - accuracy: 0.6667\nEpoch 29/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.9244 - accuracy: 0.6667\nEpoch 30/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.9039 - accuracy: 0.6667\nEpoch 31/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.8896 - accuracy: 0.6667\nEpoch 32/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.8700 - accuracy: 0.6667\nEpoch 33/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.8518 - accuracy: 0.6667\nEpoch 34/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.8403 - accuracy: 0.6667\nEpoch 35/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.8213 - accuracy: 0.6667\nEpoch 36/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.8071 - accuracy: 0.6667\nEpoch 37/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.7944 - accuracy: 0.6667\nEpoch 38/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.7772 - accuracy: 0.6667\nEpoch 39/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.7662 - accuracy: 0.6667\nEpoch 40/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.7515 - accuracy: 0.6667\nEpoch 41/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.7380 - accuracy: 0.6667\nEpoch 42/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.7271 - accuracy: 0.6667\nEpoch 43/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.7130 - accuracy: 0.6667\nEpoch 44/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.7002 - accuracy: 0.6667\nEpoch 45/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.6667\nEpoch 46/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.6774 - accuracy: 0.6667\nEpoch 47/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.6641 - accuracy: 0.6667\nEpoch 48/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.6515 - accuracy: 0.6667\nEpoch 49/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.6405 - accuracy: 0.6667\nEpoch 50/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.6309 - accuracy: 0.6667\nEpoch 51/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.6225 - accuracy: 0.6667\nEpoch 52/200\n1/1 [==============================] - ETA: 0s - loss: 0.6161 - accuracy: 0.831/1 [==============================] - 0s 1ms/step - loss: 0.6161 - accuracy: 0.8333\nEpoch 53/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.5991 - accuracy: 0.8333\nEpoch 54/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.8333\nEpoch 55/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.8333\nEpoch 56/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.5655 - accuracy: 0.8333\nEpoch 57/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.5511 - accuracy: 0.8333\nEpoch 58/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.5404 - accuracy: 0.8333\nEpoch 59/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.5302 - accuracy: 0.8333\nEpoch 60/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.8333\nEpoch 61/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.5045 - accuracy: 0.8333\nEpoch 62/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.4940 - accuracy: 0.8333\nEpoch 63/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.4810 - accuracy: 0.8333\nEpoch 64/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.4687 - accuracy: 0.8333\nEpoch 65/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.4583 - accuracy: 0.8333\nEpoch 66/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.4464 - accuracy: 0.8333\nEpoch 67/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.4347 - accuracy: 0.8333\nEpoch 68/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.4253 - accuracy: 0.8333\nEpoch 69/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.4149 - accuracy: 0.8333\nEpoch 70/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.8333\nEpoch 71/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8333\nEpoch 72/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8333\nEpoch 73/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.3794 - accuracy: 0.8333\nEpoch 74/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8333\nEpoch 75/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8333\nEpoch 76/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.8333\nEpoch 77/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.3517 - accuracy: 0.8333\nEpoch 78/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8333\nEpoch 79/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8333\nEpoch 80/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.8333\nEpoch 81/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8333\nEpoch 82/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8333\nEpoch 83/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.3162 - accuracy: 0.8333\nEpoch 84/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.3111 - accuracy: 0.8333\nEpoch 85/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.8333\nEpoch 86/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.3001 - accuracy: 0.8333\nEpoch 87/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2950 - accuracy: 0.8333\nEpoch 88/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8333\nEpoch 89/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2844 - accuracy: 0.8333\nEpoch 90/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.2794 - accuracy: 0.8333\nEpoch 91/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8333\nEpoch 92/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2688 - accuracy: 0.8333\nEpoch 93/200\n1/1 [==============================] - 0s 3ms/step - loss: 0.2636 - accuracy: 0.8333\nEpoch 94/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.2583 - accuracy: 0.8333\nEpoch 95/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2529 - accuracy: 0.8333\nEpoch 96/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2475 - accuracy: 0.8333\nEpoch 97/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2421 - accuracy: 0.8333\nEpoch 98/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2365 - accuracy: 0.8333\nEpoch 99/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2311 - accuracy: 1.0000\nEpoch 100/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2255 - accuracy: 1.0000\nEpoch 101/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 1.0000\nEpoch 102/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2143 - accuracy: 1.0000\nEpoch 103/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.2088 - accuracy: 1.0000\nEpoch 104/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 1.0000\nEpoch 105/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 1.0000\nEpoch 106/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 1.0000\nEpoch 107/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 1.0000\nEpoch 108/200\n1/1 [==============================] - 0s 998us/step - loss: 0.1808 - accuracy: 1.0000\nEpoch 109/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 1.0000\nEpoch 110/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.1701 - accuracy: 1.0000\nEpoch 111/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 1.0000\nEpoch 112/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1599 - accuracy: 1.0000\nEpoch 113/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 1.0000\nEpoch 114/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 1.0000\nEpoch 115/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 1.0000\nEpoch 116/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.1414 - accuracy: 1.0000\nEpoch 117/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 1.0000\nEpoch 118/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 1.0000\nEpoch 119/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 1.0000\nEpoch 120/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 1.0000\nEpoch 121/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 1.0000\nEpoch 122/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 1.0000\nEpoch 123/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 1.0000\nEpoch 124/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.1120 - accuracy: 1.0000\nEpoch 125/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 1.0000\nEpoch 126/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 1.0000\nEpoch 127/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 1.0000\nEpoch 128/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 1.0000\nEpoch 129/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0977 - accuracy: 1.0000\nEpoch 130/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 1.0000\nEpoch 131/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0927 - accuracy: 1.0000\nEpoch 132/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 1.0000\nEpoch 133/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 1.0000\nEpoch 134/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 1.0000\nEpoch 135/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0837 - accuracy: 1.0000\nEpoch 136/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 1.0000\nEpoch 137/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 1.0000\nEpoch 138/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 1.0000\nEpoch 139/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 1.0000\nEpoch 140/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 1.0000\nEpoch 141/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 1.0000\nEpoch 142/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 1.0000\nEpoch 143/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0696 - accuracy: 1.0000\nEpoch 144/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 1.0000\nEpoch 145/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 1.0000\nEpoch 146/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 1.0000\nEpoch 147/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 1.0000\nEpoch 148/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 1.0000\nEpoch 149/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0618 - accuracy: 1.0000\nEpoch 150/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 1.0000\nEpoch 151/200\n1/1 [==============================] - 0s 0s/step - loss: 0.0597 - accuracy: 1.0000\nEpoch 152/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 1.0000\nEpoch 153/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 1.0000\nEpoch 154/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 1.0000\nEpoch 155/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0559 - accuracy: 1.0000\nEpoch 156/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0550 - accuracy: 1.0000\nEpoch 157/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 1.0000\nEpoch 158/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 1.0000\nEpoch 159/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 1.0000\nEpoch 160/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0519 - accuracy: 1.0000\nEpoch 161/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 1.0000\nEpoch 162/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 1.0000\nEpoch 163/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 1.0000\nEpoch 164/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 1.0000\nEpoch 165/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0485 - accuracy: 1.0000\nEpoch 166/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 1.0000\nEpoch 167/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0472 - accuracy: 1.0000\nEpoch 168/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0467 - accuracy: 1.0000\nEpoch 169/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 1.0000\nEpoch 170/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 1.0000\nEpoch 171/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0450 - accuracy: 1.0000\nEpoch 172/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0444 - accuracy: 1.0000\nEpoch 173/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0439 - accuracy: 1.0000\nEpoch 174/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 1.0000\nEpoch 175/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0429 - accuracy: 1.0000\nEpoch 176/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 1.0000\nEpoch 177/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0419 - accuracy: 1.0000\nEpoch 178/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0415 - accuracy: 1.0000\nEpoch 179/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0410 - accuracy: 1.0000\nEpoch 180/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0406 - accuracy: 1.0000\nEpoch 181/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 1.0000\nEpoch 182/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0397 - accuracy: 1.0000\nEpoch 183/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 1.0000\nEpoch 184/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 1.0000\nEpoch 185/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0385 - accuracy: 1.0000\nEpoch 186/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 1.0000\nEpoch 187/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0377 - accuracy: 1.0000\nEpoch 188/200\n1/1 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 1.0000\nEpoch 189/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 1.0000\nEpoch 190/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 1.0000\nEpoch 191/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 1.0000\nEpoch 192/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0359 - accuracy: 1.0000\nEpoch 193/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0355 - accuracy: 1.0000\nEpoch 194/200\n1/1 [==============================] - 0s 0s/step - loss: 0.0352 - accuracy: 1.0000\nEpoch 195/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 1.0000\nEpoch 196/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0345 - accuracy: 1.0000\nEpoch 197/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 1.0000\nEpoch 198/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0339 - accuracy: 1.0000\nEpoch 199/200\n1/1 [==============================] - 0s 1ms/step - loss: 0.0335 - accuracy: 1.0000\nEpoch 200/200\n1/1 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 1.0000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2cbb8fefcc8>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "model.compile(loss= 'categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "model.fit(x,y,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[3.1467823e-03, 3.3309341e-03, 3.9268788e-02, 9.4730985e-01,\n         6.9435807e-03],\n        [9.0610487e-03, 7.8992610e-04, 9.5688099e-01, 2.9319841e-02,\n         3.9481181e-03],\n        [9.7389054e-01, 1.3075087e-03, 7.9952935e-03, 1.0488108e-03,\n         1.5757930e-02],\n        [1.2912435e-02, 3.0295732e-03, 3.4067556e-03, 8.9128432e-04,\n         9.7975999e-01],\n        [5.0189905e-03, 1.6118433e-02, 1.3648822e-03, 1.1943213e-03,\n         9.7630346e-01],\n        [1.0110065e-03, 9.7210127e-01, 1.1962255e-03, 6.8613573e-04,\n         2.5005359e-02]]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "predictions = model.predict(x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2 3 2 0 4 4] hihell\n[3 2 0 4 4 1] ihello\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\n결과 :\\n[2 3 2 0 4 4] hihell\\n[0 0 0 1 1 1] eeeooo\\n좋은 모델이 아님!\\n'"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "for i, pred in enumerate(predictions):\n",
    "    xindex = np.argmax(x[i], axis=1)\n",
    "    xstr = [charSet[j] for j in xindex]\n",
    "    print(xindex,\"\".join(xstr))\n",
    "\n",
    "    ind = np.argmax(pred, axis=1)\n",
    "    res = [charSet[j] for j in ind]\n",
    "    print(ind,''.join(res))\n",
    "\n",
    "\"\"\"\n",
    "첫번째 모델 결과 :\n",
    "[2 3 2 0 4 4] hihell\n",
    "[0 0 0 1 1 1] eeeooo\n",
    "좋은 모델이 아님!\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "딥하게 구조한 모델 결과 :\n",
    "[2 3 2 0 4 4] hihell\n",
    "[3 2 0 4 4 1] ihello\n",
    "정확하게 예측함\n",
    "\"\"\""
   ]
  }
 ]
}