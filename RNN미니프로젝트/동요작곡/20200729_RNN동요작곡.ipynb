{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595994557224",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils import np_utils, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나비야 동요 음계 데이터\n",
    "code2idx = {'c4':0, 'd4':1, 'e4':2, 'f4':3, 'g4':4, 'a4':5, 'b4':6,\n",
    "            'c8':7, 'd8':8, 'e8':9, 'f8':10, 'g8':11, 'a8':12, 'b8':13}\n",
    "idx2code = {0:'c4', 1:'d4', 2:'e4', 3:'f4', 4:'g4', 5:'a4', 6:'b4',\n",
    "            7:'c8', 8:'d8', 9:'e8', 10:'f8', 11:'g8', 12:'a8', 13:'b8'}\n",
    "seq = ['g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'd8', 'e8', 'f8', 'g8', 'g8', 'g4',\n",
    "       'g8', 'e8', 'e8', 'e8', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4',\n",
    "       'd8', 'd8', 'd8', 'd8', 'd8', 'e8', 'f4', 'e8', 'e8', 'e8', 'e8', 'e8', 'f8', 'g4',\n",
    "       'g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2dataset(seq, windoeSize):\n",
    "    dataset=[]\n",
    "    for i in range(len(seq)- windoeSize):\n",
    "        subset = seq[i : (i+windoeSize+1)]\n",
    "        dataset.append([code2idx[item] for item in subset])\n",
    "    return np.array(dataset)\n",
    "\n",
    "dataset=seq2dataset(seq, windoeSize=4) #windowSize : 한번 학습할때 입력되는 음표갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[11,  9,  2, 10,  8],\n       [ 9,  2, 10,  8,  1],\n       [ 2, 10,  8,  1,  7],\n       [10,  8,  1,  7,  8],\n       [ 8,  1,  7,  8,  9],\n       [ 1,  7,  8,  9, 10],\n       [ 7,  8,  9, 10, 11],\n       [ 8,  9, 10, 11, 11],\n       [ 9, 10, 11, 11,  4],\n       [10, 11, 11,  4, 11],\n       [11, 11,  4, 11,  9],\n       [11,  4, 11,  9,  9],\n       [ 4, 11,  9,  9,  9],\n       [11,  9,  9,  9, 10],\n       [ 9,  9,  9, 10,  8],\n       [ 9,  9, 10,  8,  1],\n       [ 9, 10,  8,  1,  7],\n       [10,  8,  1,  7,  9],\n       [ 8,  1,  7,  9, 11],\n       [ 1,  7,  9, 11, 11],\n       [ 7,  9, 11, 11,  9],\n       [ 9, 11, 11,  9,  9],\n       [11, 11,  9,  9,  2],\n       [11,  9,  9,  2,  8],\n       [ 9,  9,  2,  8,  8],\n       [ 9,  2,  8,  8,  8],\n       [ 2,  8,  8,  8,  8],\n       [ 8,  8,  8,  8,  8],\n       [ 8,  8,  8,  8,  9],\n       [ 8,  8,  8,  9,  3],\n       [ 8,  8,  9,  3,  9],\n       [ 8,  9,  3,  9,  9],\n       [ 9,  3,  9,  9,  9],\n       [ 3,  9,  9,  9,  9],\n       [ 9,  9,  9,  9,  9],\n       [ 9,  9,  9,  9, 10],\n       [ 9,  9,  9, 10,  4],\n       [ 9,  9, 10,  4, 11],\n       [ 9, 10,  4, 11,  9],\n       [10,  4, 11,  9,  2],\n       [ 4, 11,  9,  2, 10],\n       [11,  9,  2, 10,  8],\n       [ 9,  2, 10,  8,  1],\n       [ 2, 10,  8,  1,  7],\n       [10,  8,  1,  7,  9],\n       [ 8,  1,  7,  9, 11],\n       [ 1,  7,  9, 11, 11],\n       [ 7,  9, 11, 11,  9],\n       [ 9, 11, 11,  9,  9],\n       [11, 11,  9,  9,  2]])"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape # (50,5)\n",
    "xTrain = dataset[:,0:4]\n",
    "yTrain = dataset[:,4]\n",
    "xTrain = xTrain/11\n",
    "yTrain = np_utils.to_categorical(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotVecSize = yTrain.shape[1] #12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 신경망으로 구현 (정확도 92%) ##############\n",
    "# 모델구성(신경망), 학습, 평가, 사용\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=4, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(oneHotVecSize, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "00\nEpoch 1804/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9000\nEpoch 1805/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9200\nEpoch 1806/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9000\nEpoch 1807/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9200\nEpoch 1808/2000\n5/5 [==============================] - 0s 8ms/step - loss: 0.1628 - accuracy: 0.8800\nEpoch 1809/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9000\nEpoch 1810/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9000\nEpoch 1811/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9000\nEpoch 1812/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1477 - accuracy: 0.9200\nEpoch 1813/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9000\nEpoch 1814/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9000\nEpoch 1815/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9000\nEpoch 1816/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9000\nEpoch 1817/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.9200\nEpoch 1818/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9000\nEpoch 1819/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.8800\nEpoch 1820/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1517 - accuracy: 0.9200\nEpoch 1821/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9000\nEpoch 1822/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9200\nEpoch 1823/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9200\nEpoch 1824/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9000\nEpoch 1825/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9200\nEpoch 1826/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9200\nEpoch 1827/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9200\nEpoch 1828/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9200\nEpoch 1829/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.8800\nEpoch 1830/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.8800\nEpoch 1831/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.8800\nEpoch 1832/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.8800\nEpoch 1833/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9000\nEpoch 1834/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9000\nEpoch 1835/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9000\nEpoch 1836/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9000\nEpoch 1837/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9200\nEpoch 1838/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9200\nEpoch 1839/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9200\nEpoch 1840/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9200\nEpoch 1841/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9000\nEpoch 1842/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9200\nEpoch 1843/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.8800\nEpoch 1844/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9200\nEpoch 1845/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9200\nEpoch 1846/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9200\nEpoch 1847/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.8800\nEpoch 1848/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9000\nEpoch 1849/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9000\nEpoch 1850/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.8800\nEpoch 1851/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9000\nEpoch 1852/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9000\nEpoch 1853/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9200\nEpoch 1854/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.8800\nEpoch 1855/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9200\nEpoch 1856/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9200\nEpoch 1857/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9000\nEpoch 1858/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9200\nEpoch 1859/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9200\nEpoch 1860/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9200\nEpoch 1861/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9000\nEpoch 1862/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9200\nEpoch 1863/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.9000\nEpoch 1864/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9200\nEpoch 1865/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9200\nEpoch 1866/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9000\nEpoch 1867/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1528 - accuracy: 0.8800\nEpoch 1868/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9000\nEpoch 1869/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9200\nEpoch 1870/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9200\nEpoch 1871/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9200\nEpoch 1872/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1476 - accuracy: 0.9200\nEpoch 1873/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9200\nEpoch 1874/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.8800\nEpoch 1875/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9200\nEpoch 1876/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9400\nEpoch 1877/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9200\nEpoch 1878/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9200\nEpoch 1879/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.8800\nEpoch 1880/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9200\nEpoch 1881/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9000\nEpoch 1882/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1506 - accuracy: 0.9200\nEpoch 1883/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1469 - accuracy: 0.9000\nEpoch 1884/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9000\nEpoch 1885/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.8800\nEpoch 1886/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9200\nEpoch 1887/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.8800\nEpoch 1888/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9200\nEpoch 1889/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1416 - accuracy: 0.9200\nEpoch 1890/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9200\nEpoch 1891/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.8800\nEpoch 1892/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1530 - accuracy: 0.8800\nEpoch 1893/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9200\nEpoch 1894/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9000\nEpoch 1895/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9000\nEpoch 1896/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9200\nEpoch 1897/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.9000\nEpoch 1898/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.8800\nEpoch 1899/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9000\nEpoch 1900/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9000\nEpoch 1901/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9000\nEpoch 1902/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.8800\nEpoch 1903/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1449 - accuracy: 0.9200\nEpoch 1904/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9200\nEpoch 1905/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9000\nEpoch 1906/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9200\nEpoch 1907/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9000\nEpoch 1908/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1469 - accuracy: 0.9200\nEpoch 1909/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9200\nEpoch 1910/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9000\nEpoch 1911/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9200\nEpoch 1912/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9200\nEpoch 1913/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1404 - accuracy: 0.9200\nEpoch 1914/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.8800\nEpoch 1915/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9200\nEpoch 1916/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1672 - accuracy: 0.9200\nEpoch 1917/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9200\nEpoch 1918/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1444 - accuracy: 0.9000\nEpoch 1919/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1507 - accuracy: 0.9000\nEpoch 1920/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1481 - accuracy: 0.9000\nEpoch 1921/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9200\nEpoch 1922/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.8800\nEpoch 1923/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.8800\nEpoch 1924/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9000\nEpoch 1925/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1529 - accuracy: 0.8800\nEpoch 1926/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9000\nEpoch 1927/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9200\nEpoch 1928/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9200\nEpoch 1929/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.9200\nEpoch 1930/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9000\nEpoch 1931/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.9000\nEpoch 1932/2000\n1/5 [=====>........................] - ETA: 0s - loss: 0.0417 - accuracy: 1.005/5 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9200\nEpoch 1933/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.8800\nEpoch 1934/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9200\nEpoch 1935/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.8800\nEpoch 1936/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9200\nEpoch 1937/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9200\nEpoch 1938/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9000\nEpoch 1939/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9200\nEpoch 1940/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9200\nEpoch 1941/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.8800\nEpoch 1942/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9000\nEpoch 1943/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9000\nEpoch 1944/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9200\nEpoch 1945/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.9200\nEpoch 1946/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9200\nEpoch 1947/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9200\nEpoch 1948/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.8600\nEpoch 1949/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1517 - accuracy: 0.9000\nEpoch 1950/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9200\nEpoch 1951/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9200\nEpoch 1952/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9000\nEpoch 1953/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1556 - accuracy: 0.9200\nEpoch 1954/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9000\nEpoch 1955/2000\n5/5 [==============================] - 0s 2ms/step - loss: 0.1512 - accuracy: 0.9200\nEpoch 1956/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9000\nEpoch 1957/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9200\nEpoch 1958/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1475 - accuracy: 0.9200\nEpoch 1959/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9000\nEpoch 1960/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9000\nEpoch 1961/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9000\nEpoch 1962/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9200\nEpoch 1963/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9000\nEpoch 1964/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9000\nEpoch 1965/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9200\nEpoch 1966/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.8800\nEpoch 1967/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.8800\nEpoch 1968/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9200\nEpoch 1969/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9200\nEpoch 1970/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9000\nEpoch 1971/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9000\nEpoch 1972/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9200\nEpoch 1973/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9200\nEpoch 1974/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9000\nEpoch 1975/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9200\nEpoch 1976/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9400\nEpoch 1977/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9200\nEpoch 1978/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9000\nEpoch 1979/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9000\nEpoch 1980/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9200\nEpoch 1981/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9200\nEpoch 1982/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9000\nEpoch 1983/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.8800\nEpoch 1984/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.8600\nEpoch 1985/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9200\nEpoch 1986/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9000\nEpoch 1987/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9200\nEpoch 1988/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.8800\nEpoch 1989/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9000\nEpoch 1990/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1399 - accuracy: 0.9200\nEpoch 1991/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9000\nEpoch 1992/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9000\nEpoch 1993/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9000\nEpoch 1994/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1475 - accuracy: 0.8600\nEpoch 1995/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9200\nEpoch 1996/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.8800\nEpoch 1997/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9200\nEpoch 1998/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9200\nEpoch 1999/2000\n5/5 [==============================] - 0s 802us/step - loss: 0.1502 - accuracy: 0.9200\nEpoch 2000/2000\n5/5 [==============================] - 0s 1ms/step - loss: 0.1523 - accuracy: 0.8800\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x25aa7cc9548>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])\n",
    "model.fit(xTrain,yTrain,epochs=2000, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2/2 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9200\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.14252984523773193, 0.9200000166893005]"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "model.evaluate(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측\n",
    "seqOut = ['g8', 'e8', 'e4', 'f8']\n",
    "preOut =model.predict(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'e8', 'e8', 'f8', 'g8', 'g8', 'g4', 'g8', 'e8', 'e8', 'e8', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4', 'd8', 'd8', 'd8', 'd8', 'd8', 'd8', 'f4', 'e8', 'e8', 'e8', 'e8', 'f8', 'f8', 'd8', 'g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4']\n"
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    idx =np.argmax(preOut[i])\n",
    "    seqOut.append(idx2code[idx])\n",
    "\n",
    "print(seqOut) # seqOut에는 최종악보가 저장된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "pygame 1.9.6\nHello from the pygame community. https://www.pygame.org/contribute.html\n"
    },
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "DirectSoundCreate: No audio device found",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-c788ed6ed61d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpygame\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmixer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmixer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmixer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmusic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sound1.mp3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmixer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmusic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: DirectSoundCreate: No audio device found"
     ]
    }
   ],
   "source": [
    "# import wave\n",
    "# !pip install pygame\n",
    "from pygame import mixer\n",
    "\n",
    "mixer.init()\n",
    "mixer.music.load('sound1.mp3')\n",
    "mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### LSTM모델로 재구성 #####################\n",
    "xTrain=dataset[:,0:4]\n",
    "yTrain=dataset[:,4]\n",
    "xTrain=np.reshape(xTrain, (50,4,1))\n",
    "xTrain=xTrain/11\n",
    "yTrain=np_utils.to_categorical(yTrain)\n",
    "oneHotVecSize=yTrain.shape[1]\n",
    "#lstm\n",
    "model=Sequential()\n",
    "model.add(LSTM(128, batch_input_shape=(1, 4, 1), stateful=True))\n",
    "model.add(Dense(oneHotVecSize, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0593 - accuracy: 0.5600\nepoch : 2\n50/50 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.7400\nepoch : 3\n50/50 [==============================] - 0s 1ms/step - loss: 1.3554 - accuracy: 0.6200\nepoch : 4\n50/50 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9000\nepoch : 5\n50/50 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9200\nepoch : 6\n50/50 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8200\nepoch : 7\n50/50 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9800\nepoch : 8\n50/50 [==============================] - 0s 2ms/step - loss: 0.7738 - accuracy: 0.7600\nepoch : 9\n50/50 [==============================] - 0s 1ms/step - loss: 0.4880 - accuracy: 0.7400\nepoch : 10\n50/50 [==============================] - 0s 1ms/step - loss: 0.3853 - accuracy: 0.8800\nepoch : 11\n50/50 [==============================] - 0s 1ms/step - loss: 0.4211 - accuracy: 0.8400\nepoch : 12\n50/50 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.8600\nepoch : 13\n50/50 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8400\nepoch : 14\n50/50 [==============================] - 0s 2ms/step - loss: 0.5711 - accuracy: 0.7800\nepoch : 15\n50/50 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 1.0000\nepoch : 16\n50/50 [==============================] - 0s 2ms/step - loss: 0.2726 - accuracy: 0.9400\nepoch : 17\n50/50 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9200\nepoch : 18\n50/50 [==============================] - 0s 1ms/step - loss: 0.1806 - accuracy: 0.9600\nepoch : 19\n50/50 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.9600\nepoch : 20\n50/50 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.9200\nepoch : 21\n50/50 [==============================] - 0s 1ms/step - loss: 0.5166 - accuracy: 0.8400\nepoch : 22\n50/50 [==============================] - 0s 2ms/step - loss: 0.1639 - accuracy: 0.9400\nepoch : 23\n50/50 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.9200\nepoch : 24\n50/50 [==============================] - 0s 2ms/step - loss: 0.7416 - accuracy: 0.7600\nepoch : 25\n50/50 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.8800\nepoch : 26\n50/50 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9200\nepoch : 27\n50/50 [==============================] - 0s 1ms/step - loss: 0.0529 - accuracy: 0.9800\nepoch : 28\n50/50 [==============================] - 0s 1ms/step - loss: 0.0388 - accuracy: 1.0000\nepoch : 29\n50/50 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 1.0000\nepoch : 30\n50/50 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 1.0000\nepoch : 31\n50/50 [==============================] - 0s 1ms/step - loss: 0.0147 - accuracy: 1.0000\nepoch : 32\n50/50 [==============================] - 0s 1ms/step - loss: 0.0129 - accuracy: 1.0000\nepoch : 33\n50/50 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 1.0000\nepoch : 34\n50/50 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 1.0000\nepoch : 35\n50/50 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 1.0000\nepoch : 36\n50/50 [==============================] - 0s 1ms/step - loss: 0.0083 - accuracy: 1.0000\nepoch : 37\n50/50 [==============================] - 0s 1ms/step - loss: 0.0076 - accuracy: 1.0000\nepoch : 38\n50/50 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000\nepoch : 39\n50/50 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000\nepoch : 40\n50/50 [==============================] - 0s 1ms/step - loss: 0.0058 - accuracy: 1.0000\nepoch : 41\n50/50 [==============================] - 0s 1ms/step - loss: 0.0054 - accuracy: 1.0000\nepoch : 42\n50/50 [==============================] - 0s 1ms/step - loss: 0.0050 - accuracy: 1.0000\nepoch : 43\n50/50 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 1.0000\nepoch : 44\n50/50 [==============================] - 0s 1ms/step - loss: 0.0044 - accuracy: 1.0000\nepoch : 45\n50/50 [==============================] - 0s 1ms/step - loss: 0.0041 - accuracy: 1.0000\nepoch : 46\n50/50 [==============================] - 0s 1ms/step - loss: 0.0038 - accuracy: 1.0000\nepoch : 47\n50/50 [==============================] - 0s 1ms/step - loss: 0.0036 - accuracy: 1.0000\nepoch : 48\n50/50 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\nepoch : 49\n50/50 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 1.0000\nepoch : 50\n50/50 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 1.0000\nepoch : 51\n50/50 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\nepoch : 52\n50/50 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000\nepoch : 53\n50/50 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000\nepoch : 54\n50/50 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000\nepoch : 55\n50/50 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000\nepoch : 56\n50/50 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\nepoch : 57\n50/50 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 1.0000\nepoch : 58\n50/50 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000\nepoch : 59\n50/50 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\nepoch : 60\n50/50 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\nepoch : 61\n50/50 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\nepoch : 62\n50/50 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000\nepoch : 63\n50/50 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000\nepoch : 64\n50/50 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\nepoch : 65\n50/50 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\nepoch : 66\n50/50 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000\nepoch : 67\n50/50 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\nepoch : 68\n50/50 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\nepoch : 69\n50/50 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\nepoch : 70\n50/50 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\nepoch : 71\n50/50 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\nepoch : 72\n50/50 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000\nepoch : 73\n50/50 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\nepoch : 74\n50/50 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\nepoch : 75\n50/50 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\nepoch : 76\n50/50 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\nepoch : 77\n50/50 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\nepoch : 78\n50/50 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 1.0000\nepoch : 79\n50/50 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 1.0000\nepoch : 80\n50/50 [==============================] - 0s 1ms/step - loss: 9.8126e-04 - accuracy: 1.0000\nepoch : 81\n50/50 [==============================] - 0s 1ms/step - loss: 9.5521e-04 - accuracy: 1.0000\nepoch : 82\n50/50 [==============================] - 0s 1ms/step - loss: 9.3027e-04 - accuracy: 1.0000\nepoch : 83\n50/50 [==============================] - 0s 2ms/step - loss: 9.0625e-04 - accuracy: 1.0000\nepoch : 84\n50/50 [==============================] - 0s 2ms/step - loss: 8.8308e-04 - accuracy: 1.0000\nepoch : 85\n50/50 [==============================] - 0s 1ms/step - loss: 8.6063e-04 - accuracy: 1.0000\nepoch : 86\n50/50 [==============================] - 0s 1ms/step - loss: 8.3883e-04 - accuracy: 1.0000\nepoch : 87\n50/50 [==============================] - 0s 2ms/step - loss: 8.1759e-04 - accuracy: 1.0000\nepoch : 88\n50/50 [==============================] - 0s 1ms/step - loss: 7.9685e-04 - accuracy: 1.0000\nepoch : 89\n50/50 [==============================] - 0s 2ms/step - loss: 7.7654e-04 - accuracy: 1.0000\nepoch : 90\n50/50 [==============================] - 0s 1ms/step - loss: 7.5664e-04 - accuracy: 1.0000\nepoch : 91\n50/50 [==============================] - 0s 1ms/step - loss: 7.3706e-04 - accuracy: 1.0000\nepoch : 92\n50/50 [==============================] - 0s 1ms/step - loss: 7.1782e-04 - accuracy: 1.0000\nepoch : 93\n50/50 [==============================] - 0s 1ms/step - loss: 6.9892e-04 - accuracy: 1.0000\nepoch : 94\n50/50 [==============================] - 0s 2ms/step - loss: 6.8033e-04 - accuracy: 1.0000\nepoch : 95\n50/50 [==============================] - 0s 1ms/step - loss: 6.6208e-04 - accuracy: 1.0000\nepoch : 96\n50/50 [==============================] - 0s 1ms/step - loss: 6.4420e-04 - accuracy: 1.0000\nepoch : 97\n50/50 [==============================] - 0s 1ms/step - loss: 6.2670e-04 - accuracy: 1.0000\nepoch : 98\n50/50 [==============================] - 0s 2ms/step - loss: 6.0964e-04 - accuracy: 1.0000\nepoch : 99\n50/50 [==============================] - 0s 1ms/step - loss: 5.9299e-04 - accuracy: 1.0000\nepoch : 100\n50/50 [==============================] - 0s 1ms/step - loss: 5.7680e-04 - accuracy: 1.0000\nepoch : 101\n50/50 [==============================] - 0s 1ms/step - loss: 5.6110e-04 - accuracy: 1.0000\nepoch : 102\n50/50 [==============================] - 0s 2ms/step - loss: 5.4589e-04 - accuracy: 1.0000\nepoch : 103\n50/50 [==============================] - 0s 1ms/step - loss: 5.3121e-04 - accuracy: 1.0000\nepoch : 104\n50/50 [==============================] - 0s 1ms/step - loss: 5.1702e-04 - accuracy: 1.0000\nepoch : 105\n50/50 [==============================] - 0s 1ms/step - loss: 5.0334e-04 - accuracy: 1.0000\nepoch : 106\n50/50 [==============================] - 0s 2ms/step - loss: 4.9019e-04 - accuracy: 1.0000\nepoch : 107\n50/50 [==============================] - 0s 1ms/step - loss: 4.7753e-04 - accuracy: 1.0000\nepoch : 108\n50/50 [==============================] - 0s 1ms/step - loss: 4.6537e-04 - accuracy: 1.0000\nepoch : 109\n50/50 [==============================] - 0s 2ms/step - loss: 4.5368e-04 - accuracy: 1.0000\nepoch : 110\n50/50 [==============================] - 0s 1ms/step - loss: 4.4247e-04 - accuracy: 1.0000\nepoch : 111\n50/50 [==============================] - 0s 2ms/step - loss: 4.3170e-04 - accuracy: 1.0000\nepoch : 112\n50/50 [==============================] - 0s 2ms/step - loss: 4.2133e-04 - accuracy: 1.0000\nepoch : 113\n50/50 [==============================] - 0s 2ms/step - loss: 4.1137e-04 - accuracy: 1.0000\nepoch : 114\n50/50 [==============================] - 0s 1ms/step - loss: 4.0177e-04 - accuracy: 1.0000\nepoch : 115\n50/50 [==============================] - 0s 1ms/step - loss: 3.9250e-04 - accuracy: 1.0000\nepoch : 116\n50/50 [==============================] - 0s 1ms/step - loss: 3.8357e-04 - accuracy: 1.0000\nepoch : 117\n50/50 [==============================] - 0s 2ms/step - loss: 3.7489e-04 - accuracy: 1.0000\nepoch : 118\n50/50 [==============================] - 0s 2ms/step - loss: 3.6648e-04 - accuracy: 1.0000\nepoch : 119\n50/50 [==============================] - 0s 2ms/step - loss: 3.5828e-04 - accuracy: 1.0000\nepoch : 120\n50/50 [==============================] - 0s 1ms/step - loss: 3.5028e-04 - accuracy: 1.0000\nepoch : 121\n50/50 [==============================] - 0s 1ms/step - loss: 3.4249e-04 - accuracy: 1.0000\nepoch : 122\n50/50 [==============================] - 0s 2ms/step - loss: 3.3484e-04 - accuracy: 1.0000\nepoch : 123\n50/50 [==============================] - 0s 2ms/step - loss: 3.2736e-04 - accuracy: 1.0000\nepoch : 124\n50/50 [==============================] - 0s 1ms/step - loss: 3.2003e-04 - accuracy: 1.0000\nepoch : 125\n50/50 [==============================] - 0s 1ms/step - loss: 3.1286e-04 - accuracy: 1.0000\nepoch : 126\n50/50 [==============================] - 0s 1ms/step - loss: 3.0583e-04 - accuracy: 1.0000\nepoch : 127\n50/50 [==============================] - 0s 1ms/step - loss: 2.9894e-04 - accuracy: 1.0000\nepoch : 128\n50/50 [==============================] - 0s 1ms/step - loss: 2.9220e-04 - accuracy: 1.0000\nepoch : 129\n50/50 [==============================] - 0s 1ms/step - loss: 2.8560e-04 - accuracy: 1.0000\nepoch : 130\n50/50 [==============================] - 0s 1ms/step - loss: 2.7918e-04 - accuracy: 1.0000\nepoch : 131\n50/50 [==============================] - 0s 1ms/step - loss: 2.7286e-04 - accuracy: 1.0000\nepoch : 132\n50/50 [==============================] - 0s 1ms/step - loss: 2.6670e-04 - accuracy: 1.0000\nepoch : 133\n50/50 [==============================] - 0s 1ms/step - loss: 2.6068e-04 - accuracy: 1.0000\nepoch : 134\n50/50 [==============================] - 0s 2ms/step - loss: 2.5481e-04 - accuracy: 1.0000\nepoch : 135\n50/50 [==============================] - 0s 1ms/step - loss: 2.4906e-04 - accuracy: 1.0000\nepoch : 136\n50/50 [==============================] - 0s 1ms/step - loss: 2.4344e-04 - accuracy: 1.0000\nepoch : 137\n50/50 [==============================] - 0s 2ms/step - loss: 2.3794e-04 - accuracy: 1.0000\nepoch : 138\n50/50 [==============================] - 0s 2ms/step - loss: 2.3255e-04 - accuracy: 1.0000\nepoch : 139\n50/50 [==============================] - 0s 1ms/step - loss: 2.2731e-04 - accuracy: 1.0000\nepoch : 140\n50/50 [==============================] - 0s 1ms/step - loss: 2.2218e-04 - accuracy: 1.0000\nepoch : 141\n50/50 [==============================] - 0s 1ms/step - loss: 2.1714e-04 - accuracy: 1.0000\nepoch : 142\n50/50 [==============================] - 0s 1ms/step - loss: 2.1220e-04 - accuracy: 1.0000\nepoch : 143\n50/50 [==============================] - 0s 1ms/step - loss: 2.0738e-04 - accuracy: 1.0000\nepoch : 144\n50/50 [==============================] - 0s 2ms/step - loss: 2.0264e-04 - accuracy: 1.0000\nepoch : 145\n50/50 [==============================] - 0s 1ms/step - loss: 1.9800e-04 - accuracy: 1.0000\nepoch : 146\n50/50 [==============================] - 0s 1ms/step - loss: 1.9345e-04 - accuracy: 1.0000\nepoch : 147\n50/50 [==============================] - 0s 1ms/step - loss: 1.8901e-04 - accuracy: 1.0000\nepoch : 148\n50/50 [==============================] - 0s 1ms/step - loss: 1.8465e-04 - accuracy: 1.0000\nepoch : 149\n50/50 [==============================] - 0s 1ms/step - loss: 1.8037e-04 - accuracy: 1.0000\nepoch : 150\n50/50 [==============================] - 0s 1ms/step - loss: 1.7618e-04 - accuracy: 1.0000\nepoch : 151\n50/50 [==============================] - 0s 1ms/step - loss: 1.7208e-04 - accuracy: 1.0000\nepoch : 152\n50/50 [==============================] - 0s 1ms/step - loss: 1.6806e-04 - accuracy: 1.0000\nepoch : 153\n50/50 [==============================] - 0s 1ms/step - loss: 1.6415e-04 - accuracy: 1.0000\nepoch : 154\n50/50 [==============================] - 0s 1ms/step - loss: 1.6032e-04 - accuracy: 1.0000\nepoch : 155\n50/50 [==============================] - 0s 1ms/step - loss: 1.5657e-04 - accuracy: 1.0000\nepoch : 156\n50/50 [==============================] - 0s 1ms/step - loss: 1.5292e-04 - accuracy: 1.0000\nepoch : 157\n50/50 [==============================] - 0s 2ms/step - loss: 1.4934e-04 - accuracy: 1.0000\nepoch : 158\n50/50 [==============================] - 0s 1ms/step - loss: 1.4589e-04 - accuracy: 1.0000\nepoch : 159\n50/50 [==============================] - 0s 1ms/step - loss: 1.4248e-04 - accuracy: 1.0000\nepoch : 160\n50/50 [==============================] - 0s 2ms/step - loss: 1.3920e-04 - accuracy: 1.0000\nepoch : 161\n50/50 [==============================] - 0s 1ms/step - loss: 1.3599e-04 - accuracy: 1.0000\nepoch : 162\n50/50 [==============================] - 0s 1ms/step - loss: 1.3289e-04 - accuracy: 1.0000\nepoch : 163\n50/50 [==============================] - 0s 1ms/step - loss: 1.2986e-04 - accuracy: 1.0000\nepoch : 164\n50/50 [==============================] - 0s 2ms/step - loss: 1.2692e-04 - accuracy: 1.0000\nepoch : 165\n50/50 [==============================] - 0s 2ms/step - loss: 1.2406e-04 - accuracy: 1.0000\nepoch : 166\n50/50 [==============================] - 0s 1ms/step - loss: 1.2128e-04 - accuracy: 1.0000\nepoch : 167\n50/50 [==============================] - 0s 1ms/step - loss: 1.1858e-04 - accuracy: 1.0000\nepoch : 168\n50/50 [==============================] - 0s 2ms/step - loss: 1.1593e-04 - accuracy: 1.0000\nepoch : 169\n50/50 [==============================] - 0s 2ms/step - loss: 1.1333e-04 - accuracy: 1.0000\nepoch : 170\n50/50 [==============================] - 0s 1ms/step - loss: 1.1079e-04 - accuracy: 1.0000\nepoch : 171\n50/50 [==============================] - 0s 1ms/step - loss: 1.0830e-04 - accuracy: 1.0000\nepoch : 172\n50/50 [==============================] - 0s 1ms/step - loss: 1.0585e-04 - accuracy: 1.0000\nepoch : 173\n50/50 [==============================] - 0s 1ms/step - loss: 1.0342e-04 - accuracy: 1.0000\nepoch : 174\n50/50 [==============================] - 0s 1ms/step - loss: 1.0106e-04 - accuracy: 1.0000\nepoch : 175\n50/50 [==============================] - 0s 2ms/step - loss: 9.8729e-05 - accuracy: 1.0000\nepoch : 176\n50/50 [==============================] - 0s 1ms/step - loss: 9.6431e-05 - accuracy: 1.0000\nepoch : 177\n50/50 [==============================] - 0s 1ms/step - loss: 9.4214e-05 - accuracy: 1.0000\nepoch : 178\n50/50 [==============================] - 0s 2ms/step - loss: 9.2095e-05 - accuracy: 1.0000\nepoch : 179\n50/50 [==============================] - 0s 2ms/step - loss: 9.0183e-05 - accuracy: 1.0000\nepoch : 180\n50/50 [==============================] - 0s 1ms/step - loss: 8.8719e-05 - accuracy: 1.0000\nepoch : 181\n50/50 [==============================] - 0s 1ms/step - loss: 8.8242e-05 - accuracy: 1.0000\nepoch : 182\n50/50 [==============================] - 0s 1ms/step - loss: 8.9365e-05 - accuracy: 1.0000\nepoch : 183\n50/50 [==============================] - 0s 1ms/step - loss: 9.1016e-05 - accuracy: 1.0000\nepoch : 184\n50/50 [==============================] - 0s 2ms/step - loss: 8.8780e-05 - accuracy: 1.0000\nepoch : 185\n50/50 [==============================] - 0s 1ms/step - loss: 8.2190e-05 - accuracy: 1.0000\nepoch : 186\n50/50 [==============================] - 0s 2ms/step - loss: 7.8335e-05 - accuracy: 1.0000\nepoch : 187\n50/50 [==============================] - 0s 2ms/step - loss: 7.6342e-05 - accuracy: 1.0000\nepoch : 188\n50/50 [==============================] - 0s 1ms/step - loss: 7.4304e-05 - accuracy: 1.0000\nepoch : 189\n50/50 [==============================] - 0s 1ms/step - loss: 7.1910e-05 - accuracy: 1.0000\nepoch : 190\n50/50 [==============================] - 0s 2ms/step - loss: 6.9395e-05 - accuracy: 1.0000\nepoch : 191\n50/50 [==============================] - 0s 2ms/step - loss: 6.6957e-05 - accuracy: 1.0000\nepoch : 192\n50/50 [==============================] - 0s 1ms/step - loss: 6.4649e-05 - accuracy: 1.0000\nepoch : 193\n50/50 [==============================] - 0s 1ms/step - loss: 6.2477e-05 - accuracy: 1.0000\nepoch : 194\n50/50 [==============================] - 0s 1ms/step - loss: 6.0425e-05 - accuracy: 1.0000\nepoch : 195\n50/50 [==============================] - 0s 2ms/step - loss: 5.8482e-05 - accuracy: 1.0000\nepoch : 196\n50/50 [==============================] - 0s 2ms/step - loss: 5.6675e-05 - accuracy: 1.0000\nepoch : 197\n50/50 [==============================] - 0s 2ms/step - loss: 5.4927e-05 - accuracy: 1.0000\nepoch : 198\n50/50 [==============================] - 0s 2ms/step - loss: 5.3313e-05 - accuracy: 1.0000\nepoch : 199\n50/50 [==============================] - 0s 2ms/step - loss: 5.1768e-05 - accuracy: 1.0000\n"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "for epoch_idx in range(200):\n",
    "    print('epoch : '+str(epoch_idx))\n",
    "    model.fit(xTrain, yTrain, epochs=1, batch_size=1, shuffle=False)\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:941 test_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:909 test_step  **\n        y_pred = self(x, training=False)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:886 __call__\n        self.name)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:180 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 4]\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-1f4b3d822d47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mxTrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1079\u001b[0m                 step_num=step):\n\u001b[0;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    616\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2419\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2420\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2772\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2773\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 2774\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2704\u001b[0m         relaxed_arg_shapes)\n\u001b[0;32m   2705\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 2706\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   2707\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:941 test_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:909 test_step  **\n        y_pred = self(x, training=False)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:886 __call__\n        self.name)\n    C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:180 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 4]\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측\n",
    "seqOut = ['g8', 'e8', 'e4', 'f8']\n",
    "preOut =model.predict(xTrain)"
   ]
  }
 ]
}